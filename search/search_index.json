{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About this website","text":"<p>Welcome to the DeepLearning.AI Companion website! I started this project to keep detailed notes and reviews for various short courses offered by DeepLearning.AI as I finished these courses. However, I soon realized that this could be a valuable resource for others who are looking to learn skills related the growing field of AI Engineering. So, I decided to convert my notes and reviews into a website that could be easily accessible to anyone interested, as well as to turn this into a community project so that others can request for features and add their own learnings.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#course-notes","title":"Course Notes","text":"<p>Explore comprehensive notes and reviews for each course, covering key concepts, practical applications, and major takeaways.</p>"},{"location":"#course-reviews","title":"Course Reviews","text":"<p>My personal review of the courses: their good and bas aspects, as well as who the traget audience should be.</p>"},{"location":"#learning-paths","title":"Learning Paths","text":"<p>Discover the best sequence of courses to take based on your interests and goals. Use our interactive flowcharts to navigate through different learning paths.</p>"},{"location":"#tags","title":"Tags","text":"<p>Filter courses by topic using our tagging system. Find the content most relevant to your needs quickly and efficiently.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>To get started, select a course from the Course Notes section or choose a learning path from the Learning Paths section to find the best sequence of courses for your interests.</p>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, suggestions, or feedback, feel free to reach out:  </p> Social Media  GitHub  Twitter  LinkedIn Handle sshkhr @sshkhr16 sshkhr <p>Happy Learning!</p>"},{"location":"catalog/","title":"Courses","text":"Name Notes Review Tags Preprocessing Unstructured Data for LLM Applications Link Coming Soon <code>Data Preprocessing</code>,  <code>RAG</code> Prompt Engineering with Llama 2 &amp; 3 Up Next Up Next <code>Intro to LLM</code>,  <code>Prompt Engineering</code>,  <code>LLM4Code</code>,  <code>Safeguards</code> Getting Started With Mistral Coming Soon Coming Soon <code>Intro to LLM</code>,  <code>RAG</code> ChatGPT Prompt Engineering for Developers Coming Soon Coming Soon <code>Prompt Engineering</code>,  <code>Intro to LLM</code> Prompt Engineering for Vision Models Coming Soon Coming Soon <code>Intro to Vision</code>,  <code>Prompt Engineering</code> Building Generative AI Applications with Gradio Coming Soon Coming Soon <code>Demos</code> LLMOps Coming Soon Coming Soon <code>MLOps</code> Pair Programming with a LLM Coming Soon Coming Soon <code>Intro to LLM</code>,  <code>LLM4Code</code> Understanding and Applying Text Embeddings Coming Soon Coming Soon <code>Text Embeddings</code> ,  <code>RAG</code>"},{"location":"learning_paths/build-a-basic-rag/","title":"Learn: How To Build a Basic RAG","text":"<pre><code>flowchart TB\n  subgraph A[Learn: fa:fa-database Data Pre-Processing]\n    A1(Preprocessing Unstructured Data for LLM Applications)\n    style A stroke:#333,stroke-width:2px\n  end\n  subgraph B[Learn: Vector Embeddings]\n    style B stroke:#333,stroke-width:2px\n    B1(Understanding and Applying Text Embeddings)\n  end\n  subgraph C[Learn: Vector Databases]\n    style C stroke:#333,stroke-width:2px\n    B2(Vector Databases: From Embeddings to Applications)\n    B3(Building Applications with Vector Databases)\n  end\n  subgraph D[Learn: Intro to LLMs]\n    style D stroke:#333,stroke-width:2px\n    C1(Getting Started With Mistral)\n    C2(Prompt Engineering with Llama 2 &amp; 3)\n    C3(Building Systems with the ChatGPT API)\n  end\n  subgraph E[Learn: Basic RAG]\n    style E stroke:#333,stroke-width:2px\n    D2(LangChain: Chat with Your Data)\n    D3(Build LLM Apps with LangChain.js)\n    D4(LangChain for LLM Application Development)\n    D5(JavaScript RAG Web Apps with LlamaIndex)\n  end\n\n  A --&gt; B\n  B --&gt; C\n  C --&gt; E\n  D --&gt; E\n\n  click A1 \"https://www.deeplearning.ai/courses/preprocessing-unstructured-data-for-llm-applications\" \"Go to Data Pre-Processing Course\"\n  click B1 \"https://www.deeplearning.ai/courses/understanding-and-applying-text-embeddings\" \"Go to Understanding and Applying Text Embeddings Course\"\n  click B2 \"https://www.deeplearning.ai/courses/vector-databases-from-embeddings-to-applications\" \"Go to Vector Databases: From Embeddings to Applications Course\"\n  click B3 \"https://www.deeplearning.ai/courses/building-applications-with-vector-databases\" \"Go to Building Applications with Vector Databases Course\"\n  click C1 \"https://www.deeplearning.ai/courses/getting-started-with-mistral\" \"Go to Getting Started With Mistral Course\"\n  click C2 \"https://www.deeplearning.ai/courses/prompt-engineering-with-llama-2-3\" \"Go to Prompt Engineering with Llama 2 &amp; 3 Course\"\n  click C3 \"https://www.deeplearning.ai/courses/building-systems-with-the-chatgpt-api\" \"Go to Building Systems with the ChatGPT API Course\"\n  click D1 \"https://www.deeplearning.ai/courses/building-and-evaluating-advanced-rag-applications\" \"Go to Building and Evaluating Advanced RAG Applications Course\"\n  click D2 \"https://www.deeplearning.ai/courses/langchain-chat-with-your-data\" \"Go to LangChain: Chat with Your Data Course\"\n  click D3 \"https://www.deeplearning.ai/courses/build-llm-apps-with-langchain-js\" \"Go to Build LLM Apps with LangChain.js Course\"\n  click D4 \"https://www.deeplearning.ai/courses/langchain-for-llm-application-development\" \"Go to LangChain for LLM Application Development Course\"\n  click D5 \"https://www.deeplearning.ai/courses/javascript-rag-web-apps-with-llamaindex\" \"Go to JavaScript RAG Web Apps with LlamaIndex Course\"\n  click D6 \"https://www.deeplearning.ai/courses/advanced-retrieval-for-ai-with-chroma\" \"Go to Advanced Retrieval for AI with Chroma Course\"\n  click D7 \"https://www.deeplearning.ai/courses/building-agentic-rag-with-llamaindex\" \"Go to Building Agentic RAG with LlamaIndex Course\"\n  click D8 \"https://www.deeplearning.ai/courses/knowledge-graphs-for-rag\" \"Go to Knowledge Graphs for RAG Course\"</code></pre>","tags":["RAG"]},{"location":"notes/building-generative-ai-applications-with-gradio/","title":"Building Generative AI Applications with Gradio","text":"<ul> <li> <p>NLP tasks with a simple interface</p> <ul> <li>Summarization/Distillation</li> <li>gr.Textbox()</li> <li>demo.launch(share=True) for sharing the app</li> <li>Named Entity Recognition: bert-ner</li> <li>allow_flagging=True for flagging bad responses</li> <li>gr.HighlightedText()</li> <li>gr.Interface(examples) to show examples of input </li> </ul> </li> <li> <p>Image interface</p> <ul> <li>Image captioning</li> <li>gr.Image()</li> <li>gr.Image(type=\"pil/numpy\") for image type. Gradio processes the image into format specified before passing to function</li> </ul> </li> <li> <p>Image generation</p> <ul> <li>input: gr.Textbox(), output: gr.Image()</li> <li>gr.Slider() for controlling numerical inputs</li> <li>gr.Blocks()<ul> <li>gr.Markdown() for title/text etc.</li> <li>gr.Row() for horizontal layout, gr.Column() for vertical layout</li> <li>gr.Button() for buttons, also need to define button.click() to associate with function to call</li> <li>gr.Accordion() for collapsible sections, set open=False to keep closed by default</li> </ul> </li> </ul> </li> <li> <p>Chat</p> <ul> <li>gr.Chatbot()<ul> <li>gr.ClearButton() and specify components() to clear</li> </ul> </li> <li>Can set up the text function to yield a response if LLM supports streaming responses</li> </ul> </li> <li> <p>demo.queue().launch() instead of demo.launch() to speed up demos</p> </li> </ul>"},{"location":"notes/chat-gpt-prompt-engineering-for-developers/","title":"ChatGPT Prompt Engineering for Developers","text":"<ul> <li>GPT 3.5 and chat completion only</li> <li>Principle 1: Clear &amp; Specific instructions<ul> <li>Use delimiters in prompt</li> <li>Ask for structured output JSON/HTML</li> <li>Ask model to check output in prompt</li> <li>Few shot prompting</li> </ul> </li> <li>Principle 2: Give model time to think<ul> <li>Specify steps required to complete a task</li> <li>Ask for output in a specified format</li> <li>Ask model to work out its own solution before rushing to a conclusion</li> </ul> </li> <li>Limitations: Hallucinations</li> <li>Iterative Prompt Development<ul> <li>Idea -&gt; Implement prompt/code -&gt; Get experimental result -&gt; Analyze Errors -&gt; Iterate</li> <li>e.g.<ul> <li>Define prompt</li> <li>Limit # of tokens/words/characters</li> <li>Ask to focus on aspects relevant to intended audience</li> </ul> </li> </ul> </li> <li>Summarizing:<ul> <li>e.g. Focus on word/character/sentence limits</li> <li>e.g. Focus on shipping and delivery</li> <li>e.g. Focus on price and value</li> <li>Exraction vs Summarization</li> </ul> </li> <li>Inferring:<ul> <li>e.g. Sentiment (positive/negative)</li> <li>e.g. Emotion types</li> <li>e.g. Information Extraction e.g. entity via structured output generation = e.g. Information Extraction e.g. topic extraction</li> </ul> </li> <li>Transforming:<ul> <li>e.g. Translation</li> <li>e.g. Tone translation</li> <li>e.g. JSON to HTML</li> <li>e.g. Spell/Grammar check (Python redlines package)</li> </ul> </li> <li>Expanding:<ul> <li>e.g. Automated reply based on sentiment</li> <li>Temperature parameter to increase/decrease creativity (i.e. variance)</li> </ul> </li> <li>Chatbot <ul> <li>System/User message</li> <li>System context specified via message</li> </ul> </li> </ul> <p>e.g.</p> <pre><code>    import panel as pn  # GUI\n    pn.extension()\n\n    panels = [] # collect display \n\n    context = [ {'role':'system', 'content':\"\"\"\n    You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n    You first greet the customer, then collects the order, \\\n    and then asks if it's a pickup or delivery. \\\n    You wait to collect the entire order, then summarize it and check for a final \\\n    time if the customer wants to add anything else. \\\n    If it's a delivery, you ask for an address. \\\n    Finally you collect the payment.\\\n    Make sure to clarify all options, extras and sizes to uniquely \\\n    identify the item from the menu.\\\n    You respond in a short, very conversational friendly style. \\\n    The menu includes \\\n    pepperoni pizza  12.95, 10.00, 7.00 \\\n    cheese pizza   10.95, 9.25, 6.50 \\\n    eggplant pizza   11.95, 9.75, 6.75 \\\n    fries 4.50, 3.50 \\\n    greek salad 7.25 \\\n    Toppings: \\\n    extra cheese 2.00, \\\n    mushrooms 1.50 \\\n    sausage 3.00 \\\n    canadian bacon 3.50 \\\n    AI sauce 1.50 \\\n    peppers 1.00 \\\n    Drinks: \\\n    coke 3.00, 2.00, 1.00 \\\n    sprite 3.00, 2.00, 1.00 \\\n    bottled water 5.00 \\\n    \"\"\"} ]  # accumulate messages\n\n\n    inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here\u2026')\n    button_conversation = pn.widgets.Button(name=\"Chat!\")\n\n    interactive_conversation = pn.bind(collect_messages, button_conversation)\n\n    dashboard = pn.Column(\n        inp,\n        pn.Row(button_conversation),\n        pn.panel(interactive_conversation, loading_indicator=True, height=300),\n    )\n\n    dashboard\n</code></pre>"},{"location":"notes/llmops/","title":"LLMOps","text":"<ul> <li> <p>Fundamentals</p> <ul> <li>MLOps: ML Development (Dev) + ML Operations (Ops)<ul> <li>integration</li> <li>testing</li> <li>releasing</li> <li>deployment</li> <li>infrastructure</li> </ul> </li> <li>MLOps framework: <ul> <li>Data ingestion -&gt; Data validation -&gt; Data transformation -&gt; Model training -&gt; Model analysis -&gt; Serving -&gt; Logging</li> <li>Job Management, Monitoring =&gt; Job Orchestration</li> </ul> </li> <li>LLMOps<ul> <li>Focus on LLMs development and managing deployment<ul> <li>experiment with foundation models</li> <li>prompt design and management</li> <li>supervised tuning</li> <li>monitoring</li> <li>evaluate generative output</li> </ul> </li> <li>LLM system design<ul> <li>Chain LLM calls</li> <li>Grounding</li> <li>Check history</li> </ul> </li> <li>e.g. LLM system <pre><code>                            prompt     response\nUser Input -&gt; Pre-processing ----&gt; LLM -----&gt; Post-processing + Responsible AI -&gt; User Output\n                 |                                  |   \n                  ----------- Grounding ------------ \n    - e.g. LLMOps pipeline\n</code></pre> <ul> <li>Data preparation and versioning -&gt; Pipeline design (supervised tuning)</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Data Preparation for Tuning a Foundation Model</p> <ul> <li>Vertex AI</li> <li>Data Warehouse<ul> <li>Large data which does not fit into memory</li> <li>e.g. Stack Overflow data<ul> <li>Tabels: Questions, Answers, Metadata</li> </ul> </li> <li>Use SQL (e.g. BigQuery) to query data</li> <li>Best practice: <ul> <li>Save large data in a data warehouse</li> <li>Filter data needed to train/test model using SQL</li> <li>Then transfer to SSD or Bucket for training</li> </ul> </li> </ul> </li> <li>Query Optimization<ul> <li>Important to optimize queries for large datasets</li> </ul> </li> <li>When dealing with lots of data, consider filtering by time (only train on newer data)</li> <li>Add instruction templates for data training to improve pefromance</li> <li>Versioning data is also important - could just use a timestamp</li> <li>JSONL is easily readable JSON format, for larger data use TFRecord or Parquet since they are more efficient to read</li> </ul> </li> <li> <p>Automation and Orchestration with Pipelines</p> <ul> <li>Kubeflow Pipelines                      -----------------                     |                 |</li> <li>Training Data -&gt; Training      Evaluation -&gt; Deployment                     |                 |                      -----------------</li> <li>Pipeline is a self-contained piece of code that represents a step in your workflow (e.g. data preprocessing, model training, model evaluation)</li> <li>Uses DSL (Domain Specific Language) to define the pipeline<ul> <li>DSL component runs a containerized environment</li> <li>When passing arguments to a component, must be keywork arguments</li> </ul> </li> <li>When dealing with large data, pass data by location (e.g. Cloud bucket location) instead of files</li> <li>Compile pipelines into a yaml file which contains instructions on how to provision, run, and finish the pipeline</li> <li>Can run pipelines using Vertex AI Pipelines (serverless layer on top of Kubernates)</li> <li>Pipelines can be shared and reused, which is a big advantage </li> </ul> </li> <li> <p>Predictions, Prompts and Safety</p> <ul> <li>Deployment: <ul> <li>Real-time could be REST API, Offline bulk use cases could be batch processing</li> <li>Model Monitoring: <ul> <li>Operational metrics: e.g. endpoint hit rate, latency, throughput, error rate</li> </ul> </li> <li>Inference scalability<ul> <li>Load test</li> <li>Controlled rollout</li> </ul> </li> <li>Latency<ul> <li>Smaller models</li> <li>Faster processors</li> <li>Regional deployment</li> </ul> </li> </ul> </li> <li>Prompting:<ul> <li>Query prompts should have same format as training data     <pre><code>    INSTRUCTION = \"\"\"\\\n        Please answer the following Stackoverflow question on Python.\\\n        Answer it like\\\n        you are a developer answering Stackoverflow questions.\\\n        Question:\n        \"\"\"\n\n    PROMPT = f\"\"\"\n            {INSTRUCTION} {QUESTION}\n            \"\"\"\n</code></pre></li> </ul> </li> <li>Safety Attributes<ul> <li>Provided by model providers</li> <li>As developer, you can add your own safety checks using the safetyAttributes values</li> <li>probabilityScore (how likely harmful) vs severityScore (how harmful)</li> </ul> </li> <li>Citation <ul> <li>VertexAI models have <code>['citationMetadata']['citations']</code> object to check for original source</li> </ul> </li> </ul> </li> </ul>"},{"location":"notes/pair-programming-with-a-llm/","title":"Pair Programming with a Large Language Model","text":"<ul> <li> <p>Getting started with PaLM</p> <ul> <li>PaLM API, Makersuite (quick prototyping), Vertex AI (scalable applications)</li> <li>PaLM has a chat/text/embedding models</li> <li>Simple prompt engineering for coding<ul> <li>e.g. \"Show me how to...\" will include instructions with code</li> <li>e.g. \"write code to...\" will generate code only</li> </ul> </li> </ul> </li> <li> <p>Using a string template</p> <ul> <li>Priming     <pre><code>prompt_template = \"\"\"\n                    {priming}\n\n                    {question}\n\n                    {decorator}\n\n                    Your solution:\n                    \"\"\"\npriming_text = \"You are an expert at writing clear, concise, Python code.\"\n# option 1\ndecorator = \"Work through it step by step, and show your work. One step per line.\"\n\n# option 2\ndecorator = \"Insert comments for each line of code.\"\n</code></pre></li> </ul> </li> <li> <p>Pair Programming Scenarios</p> <ul> <li>Improve existing code e.g.     <pre><code>prompt_template = \"\"\"\nI don't think this code is the best way to do it in Python, can you help me?\n\n{question}\n\nPlease explore multiple ways of solving the problem, \nand tell me which is the most Pythonic\n\"\"\"\n</code></pre></li> <li>Simplify existing code     <pre><code>prompt_template = \"\"\"\nCan you please simplify this code for a linked list in Python?\n\n{question}\n\nExplain in detail what you did to modify it, and why.\n\"\"\"\n</code></pre></li> <li>Write test code     <pre><code>prompt_template = \"\"\"\nCan you please create test cases in code for this Python code?\n\n{question}\n\nExplain in detail what these test cases are designed to achieve.\n\"\"\"\n</code></pre></li> <li>Make code more efficient     <pre><code>prompt_template = \"\"\"\nCan you please make this code more efficient?\n\n{question}\n\nExplain in detail what you changed and why.\n\"\"\"\n</code></pre></li> <li>Debug your code     <pre><code>prompt_template = \"\"\"\nCan you please help me debug this code?\n\n{question}\n\nExplain in detail what you found and why it was a bug.\n\"\"\"\n</code></pre></li> <li>Double check LLM code cause it is often slightly wrong</li> </ul> </li> <li> <p>Technical Debt</p> <ul> <li>Issue where complicated code is written, and often handed down to other developers</li> <li>e.g. For explaining previously written code     <pre><code>prompt_template = \"\"\"\n                Can you please explain how this code works?\n\n                {question}\n\n                Use a lot of detail and make it as clear as possible.\n                \"\"\"\n</code></pre></li> <li>e.g. For writing documentation     <pre><code>prompt_template = \"\"\"\n                    Please write technical documentation for this code and \\n\n                    make it easy for a non swift developer to understand:\n\n                    {question}\n\n                    Output the results in markdown\n                    \"\"\"\n</code></pre></li> </ul> </li> </ul>"},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/","title":"Notes: Preprocessing Unstructured Data for LLM Applications","text":"","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#01-introduction","title":"01. Introduction","text":"<p>Course covers preprocessing unstructured data from various file formats, getting it into a structured format, and utilizing the data in LLM applications for both semantic search (text similarity based) and hybrid search (semantic + metadata based information retrieval).</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#02-overview-of-llm-data-preprocessing","title":"02. Overview of LLM Data Preprocessing","text":"<ul> <li>Retrieval Augmented Generation (RAG): Grounding LLM responses on validated external information</li> <li>Contextual Integration: RAG apps load context into a datbase, retrieve relevant content, and insert it into a prompt</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#preprocessing-unstructured-data","title":"Preprocessing Unstructured Data","text":"<ul> <li>Document Content: Extracting text from documents. Used for keyword or similarity search</li> <li>Document Elements: Building blocks of documents. Useful for filtering and chunking in RAG<ul> <li>e.g. Title, Narrative Text, List Item, Table, Image</li> </ul> </li> <li>Document Metadata: Extracting metadata from documents. Used for filtering in hybrid search, or identifying source of a response<ul> <li>e.g. Filename, File Type, Page Number, Section</li> </ul> </li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#why-is-data-preprocessing-hard","title":"Why is Data Preprocessing Hard?","text":"<ul> <li>Content Cues: Different element types in an HTML versus a Markdown document</li> <li>Standardization Need: Need to standardize data across different document types for pipelines</li> <li>Extracttion Variability: Different requirements for extracting data from different document types e.g. forms vs journal articles</li> <li>Metadata Insight: Extracting metadata requires understanding of document structure</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#03-normalizing-document-content","title":"03. Normalizing Document Content","text":"","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#why-normalize","title":"Why Normalize?","text":"<p>We want data in common format so that your LLM pipeline is source agnostic. Benefits:</p> <ul> <li>Normalization Benefit: <ul> <li>Filter out the unwanted elements (e.g. headers, footers)</li> <li>Chunk elements by sections</li> </ul> </li> <li>Reduced Cost: <ul> <li>Downstream tasks like chunking easier on normalized data</li> <li>Can chunk later without reprocessing documents</li> </ul> </li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#why-serialize","title":"Why Serialize?","text":"<p>Once data is normalized, we can serialize it to use again later. JSON is a suiable serialization format:</p> <ul> <li>Common structure, well understood</li> <li>Standard HTTP response format</li> <li>Easy to use in different programming languages</li> <li>Can be converted into JSONL for streaming use cases</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#preprocessing-html","title":"Preprocessing HTML","text":"<ul> <li>LLM Relevance: Integrate fresh internet content into LLM rcontext to maintain relevance.</li> <li>HTML Understanding: Use existing structure from HTML tags to extract document structure, use NLP to understand text content<ul> <li>e.g. Long content within <p> tags is probably content, short capitalized text is probably a section title <li>Data Extraction and Categorization: Analyze HTML elements to extract and organize web content</li> <pre><code>filename = \"example_files/medium_blog.html\"\nelements = partition_html(filename=filename)\n\nelement_dict = [el.to_dict() for el in elements]\nexample_output = json.dumps(element_dict[11:15], indent=2)\nprint(example_output)\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#preprocessing-powerpoint","title":"Preprocessing Powerpoint","text":"<ul> <li>Professionals Use Cases: Extracting content from Powerpoint slides useful for LLM applications in business use cases</li> <li>Extraction Process: Under the hood <code>.pptx</code> are combination of <code>.xml</code> files. Can exploit known structure to extract content</li> <li>Tool Utilization: Use python libraries like <code>pptx</code> library to extract content from Powerpoint slides</li> </ul> <pre><code>filename = \"example_files/msft_openai.pptx\"\nelements = partition_pptx(filename=filename)\nelement_dict = [el.to_dict() for el in elements]\nJSON(json.dumps(element_dict[:], indent=2))\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#preprocessing-pdfs","title":"Preprocessing PDFs","text":"<p>More complex than HTML or Powerpoints</p> <ul> <li>PDF Characterstics:<ul> <li>Consistent format across devices</li> <li>Complex structure with diverse layout (images, tables, text in various orders)</li> </ul> </li> <li>Extraction Process: <ul> <li>Need to facilitate text and element extraction</li> <li>Need to preserve context and layout</li> </ul> </li> <li>Advanced: Use OCR and transformer-based approaches - use visual information to extract content</li> </ul> <pre><code>filename = \"example_files/CoT.pdf\"\nwith open(filename, \"rb\") as f:\n    files=shared.Files(\n        content=f.read(), \n        file_name=filename,\n    )\n\nreq = shared.PartitionParameters(\n    files=files,\n    strategy='hi_res',\n    pdf_infer_table_structure=True,\n    languages=[\"eng\"],\n)\ntry:\n    # Model based workload\n    resp = s.general.partition(req)\n    print(json.dumps(resp.elements[:3], indent=2))\nexcept SDKError as e:\n    print(e)\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#04-metadata-extraction-and-chunking","title":"04. Metadata Extraction and Chunking","text":"<p>Metadata supports hybrid search</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#what-is-metadata","title":"What is Metadata?","text":"<ul> <li>Document Details: Additional information about content extracted from source document</li> <li>Source Identification: Data about document itself e.g. filename, source URL, filetype</li> <li>Structural Information: Metadata extracted from document structure e.g. element type, heirarchy information, section information, page number</li> <li>Search enhancement: Metadata can be used to filter search results for hybrid search (semantic + traditional search)</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#semantic-search","title":"Semantic Search","text":"<ul> <li>Goal: Given input text, find semantically similar documents from a corpus (e.g. for use in prompt templates as context)</li> <li>Vector Embeddings: Convert text into vectors, then use similarity measures e.g. cosine similarity to find similar documents</li> <li>Vector Database: Databse optimized for storing vector embeddings and performing similarity search<ul> <li>Load: Insert vectors into database, along with source document (or pointer to it)</li> </ul> </li> <li>Prompt Templates: Insert relevant context into the token context window for an LLM input<ul> <li>Query Embed: Emebed the input into the vector, perform similarity search</li> <li>Compare and Retrieve: Retrieve k most similar documents, insert into prompt template</li> </ul> </li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#hybrid-search","title":"Hybrid Search","text":"<p>When does semantic search fail?</p> <ul> <li>Too many results: Sometimes similarity search returns too many matches, e.g. when many documents on similar topic</li> <li>Most recent information: Users may care about most recent information, not most semantically similar</li> <li>Loss of important information: Semantic search may not consider all relevant information, such as section information</li> </ul> <p>Why hybrid search?</p> <ul> <li>Hybrid Search: Combine semantic search with traditional information retrieval, such as filtering and keyword search</li> <li>Filtering: Use metadata to filter search results</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#metadata-extraction-and-chunking","title":"Metadata Extraction and Chunking","text":"<p>e.g. Extracting metadata from an ePUB file for a book</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#filerting-elements","title":"Filerting elements","text":"<p>e.g. Filtering elements from a response to find those that are titles and contain the word \"hockey\"</p> <pre><code>[x for x in resp.elements if x['type'] == 'Title' and 'hockey' in x['text'].lower()]\n</code></pre> <p>e.g. Filtering element IDs for elements corresponding to chapters. All child elements of this element will be contents of the chapter.</p> <pre><code>chapter_ids = {}\nfor element in resp.elements:\n    for chapter in chapters:\n        if element[\"text\"] == chapter and element[\"type\"] == \"Title\":\n            chapter_ids[element[\"element_id\"]] = chapter\n            break\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#load-documents-into-a-vector-database","title":"Load documents into a vector database","text":"<p>ChromaDB is an in-memory vector database.</p> <pre><code>client = chromadb.PersistentClient(path=\"chroma_tmp\", settings=chromadb.Settings(allow_reset=True))\nclient.reset()\n\ncollection = client.create_collection(\n    name=\"winter_sports\",\n    metadata={\"hnsw:space\": \"cosine\"} # metadata for vector emebdding\n)\n\nfor element in resp.elements:\n    parent_id = element[\"metadata\"].get(\"parent_id\")\n    chapter = chapter_ids.get(parent_id, \"\")\n    collection.add(\n        documents=[element[\"text\"]],\n        ids=[element[\"element_id\"]],\n        metadatas=[{\"chapter\": chapter}]\n    )\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#perform-hybrid-search-with-metadata","title":"Perform Hybrid Search with metadata","text":"<pre><code>result = collection.query(\n    query_texts=[\"How many players are on a team?\"],\n    n_results=2,\n    where={\"chapter\": \"ICE-HOCKEY\"},\n)\nprint(json.dumps(result, indent=2))\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#chunking","title":"Chunking","text":"<ul> <li>Chunking Necessity: Vector databases need documents to be split into chunks for retrieval and prompt generation</li> <li>Query Result Variability: Document chunking quality determines quality of query results</li> <li>Chunking Process: <ul> <li>Most straightforward: Split document into fixed-size chunks</li> <li>By atomic elements: Rather than splitting raw text, split by document elements.<ul> <li>Results in more coherent chunks</li> <li>e.g. combining content under same section into the same chunk</li> </ul> </li> </ul> </li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#chunking-by-elements","title":"Chunking By Elements","text":"<ol> <li>First, break down documents into atomic elements.</li> <li>Combine elements into a chunk, until you reach a character or token threshold.</li> <li>Apply conditions for starting a new chunk, e.g. when a new section starts, or when a section metadata changes.</li> <li>Optionally, combine small chunks into larger chunks so that they are sufficiently large for effective semantic search.</li> </ol> <pre><code>elements = dict_to_elements(resp.elements)\n\nchunks = chunk_by_title(\n    elements,\n    combine_text_under_n_chars=100,\n    max_characters=3000,\n)\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#05-preprocessing-pdfs-and-images","title":"05. Preprocessing PDFs and Images","text":"<p>These types of documents require model based processing (i.e. using an ML model). This lecture covers document layout detection, and using vision transformers for document processing.</p> <ul> <li>Rules based parsing: Extracting text from a document based on known structure (e.g. HTML, Powerpoint)</li> <li>Visual information based parsing: Extracting text from a document based on visual information (e.g. PDF, Images)</li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#document-image-analysis","title":"Document Image Analysis","text":"<p>Extract formatting information and text from raw image of the document. - Document Layout Detection - Vision Transformers</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#document-layout-detection","title":"Document Layout Detection","text":"<p>Use object detection to draw and label bounding boxes around elements in the document image.</p> <ol> <li>Detection: Identify and classify a bounding box using a Vision model like YOLOX or Detectron2.</li> <li>OCR: Extract text from the bounding box using an OCR model like Tesseract or EasyOCR.</li> <li>Direct Extraction: For some documents like PDFs, text can be exracted directly from the dcoument without OCR.</li> </ol>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#vision-transformers","title":"Vision Transformers","text":"<p>Document image as input (+ a text prompt - optionally) and produce a text representation of a structured output (e.g. JSON).</p> <ol> <li>Architecture: Use a Vision Transformer model like DONUT (Document Understanding Transformer).</li> <li>Direct Extraction: Extract text from the document image using the model, no OCR required.</li> <li>Structured Output: Model can be trained to output structured JSON representation.</li> </ol>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#comparison-of-document-analysis-approaches","title":"Comparison of Document Analysis Approaches","text":"Document Layout Models Vision Transformers Advantages - Fixed set of element types - More flexible for non-standard docs (e.g. forms) - Get bounding box info - More adaptable for new ontologies Disadvantages - Two model calls (detection + ocr) - Generative model (prone to hallucination, repetition) - Less flexible - Computationally Expensive <pre><code>with open(filename, \"rb\") as f:\n    files=shared.Files(\n        content=f.read(),\n        file_name=filename,\n    )\n\nreq = shared.PartitionParameters(\n    files=files,\n    strategy=\"hi_res\",\n    hi_res_model_name=\"yolox\",\n)\n\ntry:\n    resp = s.general.partition(req)\n    dld_elements = dict_to_elements(resp.elements)\nexcept SDKError as e:\n    print(e)\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#06-extracting-tables","title":"06. Extracting Tables","text":"<p>Most LLM/RAG use cases focus on text content withing documents.</p> <ul> <li>Structured Data: Some industries (e.g. finance, insurance, etc.) deal heavily with structured data embedded withing structured documents.</li> <li>Table Extraction: For QA over tables, we first need to extract tables from documents.<ul> <li>Some documents contain inherent table structure information (e.g. HTML, Word, Excel).</li> <li>For other documents, we need to extract tables (e.g. PDFs, Images).</li> </ul> </li> </ul>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#table-extraction-approaches","title":"Table Extraction Approaches","text":"<ul> <li>Table Transformers</li> <li>Vision Transformers</li> <li>OCR Post-Processing</li> </ul> <p>After processing, keep table data in a structured format (e.g. HTML) for downstream processing.</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#table-transformers","title":"Table Transformers","text":"<p>Table Transformers: Model trained to extract tables from document images called Tableformer. Steps:     1. Detection: Detect table regions in the document image using document layout model.     2. Extraction: Extract table using tableformer, which returns HTML representation of the table.</p> <p>Pros: Can trace cells back to original bounding box. Cons: Requires two models (layout detection + table extraction). Expensive.</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#vision-transformers_1","title":"Vision Transformers","text":"<p>Use vision transformers, but trained with HTML response.</p> <p>Pros: One model call; more flexible; can be prompted with text. Cons: Generative - prone to hallucination; no bounding boxes so limited grounding.</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#ocr-post-processing","title":"OCR Post-Processing","text":"<p>OCR the table, then use output patterns in the OCR output to build the table.</p> <p>Pros: Fast; works well for tables which are not too complicated. Cons: Requires rule-based parsing; Less flexible; No bounding box.</p> <pre><code>filename = \"example_files/embedded-images-tables.pdf\"\n\nwith open(filename, \"rb\") as f:\n    files=shared.Files(\n        content=f.read(),\n        file_name=filename,\n    )\n\nreq = shared.PartitionParameters(\n    files=files,\n    strategy=\"hi_res\",\n    hi_res_model_name=\"yolox\",\n    skip_infer_table_types=[],\n    pdf_infer_table_structure=True,\n)\n\ntry:\n    resp = s.general.partition(req)\n    elements = dict_to_elements(resp.elements)\nexcept SDKError as e:\n    print(e)\n\ntables = [el for el in elements if el.category == \"Table\"]\n\ntable_html = tables[0].metadata.text_as_html\n\nfrom io import StringIO \nfrom lxml import etree\n\nparser = etree.XMLParser(remove_blank_text=True)\nfile_obj = StringIO(table_html)\ntree = etree.parse(file_obj, parser)\nprint(etree.tostring(tree, pretty_print=True).decode())\n</code></pre>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#06-build-your-own-rag-bot","title":"06. Build your own RAG Bot","text":"<pre><code>%%{init: {'flowchart' : {'curve' : 'stepAfter'}}}%%\n\nflowchart-elk TD\n    subgraph Unstructured Data\n        A1[HTML]\n        A2[PowerPoint]\n        A3[PDF]\n    end\n    subgraph RAG System\n        B[Vector Database]\n        C[Prompt: Context + Query]\n        D[LLM]\n        E[User]\n    end\n    A1 --&gt;|Extract| B\n    A2 --&gt;|Extract| B\n    A3 --&gt;|Extract| B\n    E --&gt;|Query| B &amp; C\n    B --&gt;|Context| C\n    C --&gt;|Prompt| D\n    D --&gt;|LLM Response| E\n</code></pre> <p><code>Code example not included because it breaks with Chroma. Chroma expects metadata to be a simpler type, and some of the metadata is a list with bounding boxes and text content in the example.</code></p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/preprocessing-unstructured-data-for-llm-applications/#07-conclusion","title":"07. Conclusion","text":"<p>Learnings: Extracting data,  metadata extraction, chunking, vector databases, semantic and hybrid search, building a RAG bot.</p>","tags":["Course Notes","Data Preprocessing","RAG"]},{"location":"notes/prompt-engineering-with-llama-2%263/","title":"Prompt Engineering with Llama 2 &amp; 3","text":"<ul> <li> <p>Llama model family:</p> <ul> <li>Llama 2: 7B, 13B, 70B </li> <li>Llama 2 Chat: instruction tuned versions of Llama 2</li> <li>Llama-cpp: Runs on regular CPUs</li> <li>Code Llama: Llama 2 fine-tuning<ul> <li>Code Llama: Long-context fine-tuning</li> <li>Code Llama Instruct: Long-context fine-tuning + instruction fine-tuning</li> <li>Code Llama Python: Python training + Long-context fine-tuning</li> </ul> </li> <li>Purple Llama: Tools + Benchmarks + Models<ul> <li>CyberSecEval: Tools and benchmarks for cybersecurity risk of LLMs</li> <li>Llama Guard: Safety classifier model</li> </ul> </li> </ul> </li> <li> <p>Llama 2:</p> <ul> <li>Hosted API service<ul> <li>Amazon Bedrock</li> <li>Anyscale</li> <li>Google Cloud</li> <li>Microsoft Azure</li> <li>Replicate</li> <li>Together.ai</li> </ul> </li> <li>Host yourself on cloud</li> <li>Download + Host yourself</li> <li>[INST]...[/INST] tags before and after prompt to let model know that it's an instruction</li> <li>Foundation model not trained for instruction following, but next token prediction<ul> <li>Optimizes for generation and not following instructions</li> </ul> </li> <li>Temperature parameter to increase/decrease creativity<ul> <li>Set to 0 to get deterministic output</li> </ul> </li> <li>Max tokens parameter to limit output length<ul> <li>Llama 2: 4096 tokens or less (sum of input + output)</li> </ul> </li> <li>By default, chats across multiple messages are treated as separate conversations, and context is not maintained</li> </ul> </li> <li> <p>To run llama 2 locally: Ollama</p> </li> <li> <p>Llama 3:</p> <ul> <li>8B Chat, 70B Chat available on Together.ai</li> </ul> </li> <li> <p>Llama is stateless</p> <ul> <li>Need to constuct multi-turn prompts manually</li> <li>use start and end tages for in-context prompting, but leave final end tag empty</li> <li>```python     chat_prompt = f\"\"\"     [INST] {prompt_1} [/INST]     {response_1}      [INST] {prompt_2} [/INST]     \"\"\" <li> <p>Prompt Engineering</p> <ul> <li>In-context learning: providing examples of what tasks you want the model to do<ul> <li>Zero-shot prompting: e.g. infer task from prompt<ul> <li>prompt = \"\"\"             Message: Hi Amit, thanks for the thoughtful birthday card!             Sentiment: ?             \"\"\"</li> </ul> </li> <li>Few-shot prompting: e.g. infer task from 2-3 examples given in prompt<ul> <li>prompt = \"\"\"             Message: Hi Dad, you're 20 minutes late to my piano recital!             Sentiment: Negative<pre><code>    Message: Can't wait to order pizza for dinner tonight\n    Sentiment: Positive\n\n    Message: Hi Amit, thanks for the thoughtful birthday card!\n    Sentiment: ?\n    \"\"\"\n- Specifying output format: \n    - e.g. \"Give a one word response\"\n    - e.g. \"Respond with either positive, negative, or neutral\"\n- Role/Persona based prompting: \n    - e.g. \"Your role is a life coach \\\n</code></pre> <p>who gives advice to people about living a good life. You attempt to provide unbiased advice.\"     - Summarization     - Providing information in the prompt         - e.g. prompt = f\"\"\"         Given the following context, who won the 2023 Women's World cup?         context: {context}         \"\"\"     - Chain-of-thought prompting         - Ask the model to \"think step by step\", or \"explain your reasoning\"         - e.g. prompt = \"\"\"         Think step by step.         Explain each intermediate step.         Only when you are done with all your steps,         provide the answer based on your intermediate steps.         \"\"\"         - Order of instructions matters             - Providing answer first and then asking for steps will often not work             - Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning.</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Comparing Llama Models</p> <ul> <li>Llama 2: 7B, 13B, 70B (base/chat), 13.5 GB, 26 GB, 138 GB </li> <li>Benchmarks: Commonsense Reasoning, World Knowledge, Reading Comprehension</li> <li>Safety/Alignment: TruthfulQA (chat &gt; base), Toxigen (chat models = 0)<ul> <li>But for fine-tuning, better to start with base</li> </ul> </li> <li>Comparing summarization</li> <li>Model-Graded Evaluations</li> <li>Reasoning<ul> <li>Smaller models tend to perform worse on reasoning tasks than larger models</li> </ul> </li> </ul> </li> <li> <p>Llama Code</p> <ul> <li>Code Llama: Llama 2 7B/13B/34B + code training <ul> <li>Code Llama 7B/13B/34B: Long-context fine-tuning</li> <li>Code Llama Instruct 7B/13B/34B: Long-context fine-tuning + instruction fine-tuning<ul> <li>Requires [INST]{prompt}[/INST] tags</li> </ul> </li> <li>Code Llama Python 7B/13B/34B: Python training + Long-context fine-tuning</li> <li>Code Filling:<ul> <li>Use  tokens <li>```python         prompt = \"\"\"             def star_rating(n):             '''             This function returns a rating given the number n,             where n is an integers from 1 to 5.             '''<pre><code>        if n == 1:\n            rating=\"poor\"\n        &lt;FILL&gt;\n        elif n == 5:\n            rating=\"excellent\"\n\n        return rating\n    \"\"\"\n```\n    - Can use CodeLLama to also ask to make code more efficient\n    - Longer context windows for code completion: 20K tokens vs 4K tokens for chat\n        - While it can be used for summarization, it's not optimized for it\n</code></pre> </li> <li> <p>Llama Guard</p> <ul> <li>Llama-2-7B fine-tuned for safety classification = Llama Guard</li> <li>Safeguarding guidelines:<ul> <li>01: Violence and Hate</li> <li>02: Sexual Content</li> <li>03: Criminal Planning</li> <li>04: Guns and Illegal Weapons</li> <li>05: Regulated or Controlled Substances</li> <li>06: Self-harm</li> </ul> </li> <li>Formatting: <ul> <li><code>python     conversation = f\"\"\"     &lt;BEGIN CONVERSATION&gt;     User: {query}     &lt;END CONVERSATION&gt;     \"\"\"</code></li> </ul> </li> <li>Can be applied to both User and Agent messages</li> </ul> </li>"},{"location":"notes/understanding-and-applying-text-embeddings/","title":"Understanding and Applying Text Embeddings","text":"<ul> <li> <p>Vector Embeddings</p> <ul> <li>A neural network based compression of text into a fixed length vector</li> <li>Similarity<ul> <li>e.g. Cosine similarity</li> <li>Useful for comparing how similar different pairs of vectors (and thus text inputs) are</li> </ul> </li> <li>Word to Sentence Embeddings<ul> <li>One way: Average embeddings for all words in a sentence<ul> <li>Basically treats sentence as a bag of words</li> </ul> </li> <li>Better to have sentence level embeddings</li> </ul> </li> </ul> </li> <li> <p>What is an embedding?</p> <ul> <li>Represent data as points in space which are semantically meaningful</li> <li>Use a transformer neural network to learn word embeddings, then take an average of context aware embeddings to get sentence embeddings</li> <li>Compute embeddings for each token (e.g. sub word). Works better for novel words and misspellings</li> <li>Learned using contrastive learning on large unsupervised corpus of texts</li> <li>Multi-modal embeddings</li> </ul> </li> <li> <p>Visualize embeddings</p> <ul> <li>Better for building intuition, not as useful for monitoring</li> <li>One way to visualize is to get a low dimensional representation of embeddings using PCA and visualize in 2D</li> <li>Another way to visualize is to use heatmaps to compare similarity between different embeddings</li> </ul> </li> <li> <p>Application of embeddings</p> <ul> <li>Clustering</li> <li>Anamoly/Outlier detection<ul> <li>Use an outlier detection algorithm e.g. Isolation Forest to find embeddings that are far from the rest</li> </ul> </li> <li>For classification (i.e. labelling)<ul> <li>Use embeddings to classify text<ul> <li>Do a normal train/test split and verify classification accuracy before using in production</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>TextGeneration with VertexAI</p> <ul> <li>Use a pre-trained LLM model to generate text + Use a prompt to guide the model<ul> <li>Generative by default</li> <li>Use a prompt to get classification, or extract information</li> </ul> </li> <li>Use temperature to creativity/randomness</li> <li>Top p: Sample from the minimum set of tokens whose probabilities add up to probability p or greater</li> <li>Top k: Sample from the top k tokens</li> <li>Sampling sequence: Top k -&gt; Top p -&gt; Temperature</li> </ul> </li> <li> <p>Question Answering: LLM text generation + embedding</p> <ul> <li>Embeddings are useful for grounding</li> <li>Semantic search with embeddings<ul> <li>Use embeddings to find similar questions<ul> <li>Use distance metrics like Euclidean distance or cosine similarity or dot product</li> </ul> </li> <li>Take new query, embed it, and find closest embeddings in the database of questions, use the answer to the closest question as context in the prompt for the LLM query<ul> <li> <p>```python     context = \"Question: \" + so_database.input_text[index_doc_cosine] +                 \"n Answer: \" + so_database.output_text[index_doc_cosine]</p> <p>prompt = f\"\"\"Here is the context: {context}             Using the relevant information from the context,             provide an answer to the query: {query}.\"             If the context doesn't provide              any relevant information,              answer with              [I couldn't find a good match in the              document database for your query]             \"\"\"             <code>``     - For questions that have no relevant context, use a default response (or some other answer flow)     - Scale with approximate nearest neighbors (ANN) search for large scale embedding databases         - e.g. scann algorithm</code>import scann`</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/","title":"Review: Preprocessing Unstructured Data for LLM Applications","text":"Warning <p>These are brief reviews for personal reference, summarizing the pros and cons of key points and methodologies covered in the course. These are subjective opinions, for a more detailed and better insight into the course, please consider enrolling in the course.</p>","tags":["Review","Data Preprocessing","RAG"]},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/#review-preprocessing-unstructured-data-for-llm-applications","title":"Review: Preprocessing Unstructured Data for LLM Applications","text":"","tags":["Review","Data Preprocessing","RAG"]},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/#overall-rating","title":"Overall Rating: \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2606\u2606","text":"<p>Overall, it's a great course for understanding various approaches to different sources of external information in unstructured data.</p>","tags":["Review","Data Preprocessing","RAG"]},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/#topics-covered","title":"Topics Covered: \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2606","text":"<p>This course does a great job of covering the need to read unstructured data, different formats that are used, how to think about extracting information from different structured and unstructured data formats, what methods are used for extraction (traditional vs ML-based), and how to use the extracted data in Retrieval Augmented Generation (RAG).</p>","tags":["Review","Data Preprocessing","RAG"]},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/#methodology-covered","title":"Methodology Covered: \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2606\u2606","text":"<p>The downside is that almost all extraction is done via API calls to unstructured data, so there is hardly any code-based explanation of what is happening. There are only occasional hints here and there about the ML models being used (e.g., YOLOX for document analysis).</p>","tags":["Review","Data Preprocessing","RAG"]},{"location":"reviews/preprocessing-unstructured-data-for-llm-applications/#recommended-audience","title":"Recommended Audience","text":"Who should take this courseWho can (probably) skip this course <ul> <li>Those new to working with unstructured data and unaware of how to structure and serialize it</li> <li>Professionals looking to understand different approaches to extracting data from structured vs unstructured data formats</li> <li>Individuals aiming to apply hybrid search methods (semantic search + metadata based filtering) in RAG applications</li> </ul> <ul> <li>Those looking for in-depth code-based explanations on the exact extraction approaches</li> <li>Researchers/Developers seeking detailed explanations of ML model based approaches for data extraction</li> <li>Individuals already proficient in Unstructured API-based data extraction</li> </ul>","tags":["Review","Data Preprocessing","RAG"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#course-notes","title":"Course Notes","text":"<ul> <li>Notes: Preprocessing Unstructured Data for LLM Applications</li> </ul>"},{"location":"tags/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Notes: Preprocessing Unstructured Data for LLM Applications</li> <li>Review: Preprocessing Unstructured Data for LLM Applications</li> </ul>"},{"location":"tags/#rag","title":"RAG","text":"<ul> <li>How To - Build a Basic RAG</li> <li>Notes: Preprocessing Unstructured Data for LLM Applications</li> <li>Review: Preprocessing Unstructured Data for LLM Applications</li> </ul>"},{"location":"tags/#review","title":"Review","text":"<ul> <li>Review: Preprocessing Unstructured Data for LLM Applications</li> </ul>"}]}